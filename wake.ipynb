{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Processing *Finnegans Wake*\n",
    "====================\n",
    "\n",
    "Below we will explore some tools in the [Python Natural Language Tool Kit](http://www.nltk.org/) and see what we can reveal of what might be a shameful choice of a warke.\n",
    "\n",
    "If you're new to *Finnegans Wake* I'll do my best to explain some of the things I'm trying to examine.\n",
    "\n",
    "Motivation\n",
    "---------------------\n",
    "When James Joyce published his infamous work of obliterature, *Finnegans Wake*, he wanted \"to keep the critics busy for 300 years\".\n",
    "\n",
    "It's been 75 years so maybe and some [Viconian thunderclaps](http://www.yourepeat.com/watch/?v=a11DEFm0WCw&start_at=347&end_at=390) later. We have new media through which we can clarify some of the obscurity of *The Wake*.\n",
    "\n",
    "Drawbacks\n",
    "---------------------\n",
    "There are admittedly drawbacks to textual analysis of *Finnegans Wake*. Principally that *The Wake* is meant to be [read out loud](https://www.youtube.com/watch?v=M8kFqiv8Vww). There's information, double, triple,..., Nth-le meaning, that's revealed when heard aloud. We're not gonna access that information heare, nor will we be able to pick up puns.\n",
    "\n",
    "Getting Started\n",
    "---------------------\n",
    "First we import the python libraries we'll be using and the text of *Finnegans Wake* itself.\n",
    "\n",
    "Note: If you're having difficulty getting these running on your machine, I recommend checking out Anaconda for OSX, which handles python package installs relatively cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'finnegans', u'wake', u'by', u'james', u'joyce', u'i', u'riverrun', u'past', u'eve', u'and', u'adam', u'from', u'swerve', u'of', u'shore', u'to', u'bend', u'of', u'bay', u'brings', u'us', u'by', u'a', u'commodius', u'vicus', u'of', u'recirculation', u'back', u'to', u'howth', u'castle', u'and', u'environs', u'sir', u'tristram', u'violer', u\"d'amores\", u\"fr'over\", u'the', u'short', u'sea', u'had', u'passen-core', u'rearrived', u'from', u'north', u'armorica', u'on', u'this', u'side']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot graphs within ipython notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Python Natural Language Tool Kit\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import Regular Expressions\n",
    "import re\n",
    "\n",
    "# Import unicode parsing functions\n",
    "import unicodedata\n",
    "\n",
    "# Define a function to import and tokenize a book from a filename,\n",
    "# Return a tuple of FULL_TEXT, TOKENIZED_TEXT\n",
    "def import_text(path):\n",
    "    full_text = open(path).read().decode('utf8')\n",
    "    tokenized_text = nltk.Text(word_tokenize(full_text))\n",
    "    # If the unicode category of a character starts with a P then it is Punctuation and we want it removed\n",
    "    tokenized_text = [w.lower() for w in tokenized_text if not unicodedata.category(w[0]).startswith('P') ]\n",
    "    return full_text, tokenized_text\n",
    "\n",
    "# Import Finnegans Wake and create token list\n",
    "wake, wake_tokens = import_text(\"res/wake.txt\")\n",
    "\n",
    "# Import a list of the most boring words in English\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Print to make sure we have only lowercase text and no punctuation tokens\n",
    "print(wake_tokens[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary Richness\n",
    "---------------------\n",
    "First things first, lets see just how linguistically rich *Finnegans Wake* is. A popular metric for vocabulary richness is ratio of unique words to total words. We'll define a function that takes a text title and its tokens and returns its richness ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======FINNEGANS WAKE======\n",
      "Number of total words: 219406\n",
      "Number of unique words: 57525\n",
      "Ratio of unique to total: 0.262185172694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def richness(title, tokens):\n",
    "    total_words = len(tokens)\n",
    "    print (\"======\" + title.upper() + \"======\")\n",
    "    print (\"Number of total words: \" + str(total_words))\n",
    "    total_unique_words = len(set(tokens))\n",
    "    print (\"Number of unique words: \" + str(total_unique_words))\n",
    "    richness_ratio = total_unique_words / total_words\n",
    "    print (\"Ratio of unique to total: \" + str(richness_ratio) + \"\\n\")\n",
    "    \n",
    "richness(\"Finnegans Wake\", wake_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, 26.7% for a 258,468-word book.\n",
    "\n",
    "Let's see that ratio for 250,000-words worth of Herman Melville' *Moby Dick* and James Joyce's *Ulysses*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======ULYSSES======\n",
      "Number of total words: 264435\n",
      "Number of unique words: 29658\n",
      "Ratio of unique to total: 0.112156106416\n",
      "\n",
      "======MOBY DICK======\n",
      "Number of total words: 211460\n",
      "Number of unique words: 18279\n",
      "Ratio of unique to total: 0.086441880261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Ulysses and create token list\n",
    "ulysses, ulysses_tokens = import_text(\"res/ulysses.txt\")\n",
    "\n",
    "# Import Moby Dick and create token list\n",
    "mobydick, mobydick_tokens = import_text(\"res/mobydick.txt\")\n",
    "\n",
    "richness(\"Ulysses\", ulysses_tokens)\n",
    "richness(\"Moby Dick\", mobydick_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ulysses* has 11.2% richness for a similar amount of words.\n",
    "\n",
    "*Moby Dick* has 8.6% richness for a similar amount of words.\n",
    "\n",
    "22.7% means *Finnegans Wake* is incredibly rich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A Portrait of the Artist*\n",
    "---------------------\n",
    "What words occur in both *Portrait of the Artist as a Young Man*, *Dubliners*, *Ulysses* and *Finnegans Wake*?\n",
    "\n",
    "We'll pull copies of all those texts in and cross-reference word occurances. Then we'll see which words occur in all four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that occur in all four of Joyce's 'Novels':\n",
      "2998\n"
     ]
    }
   ],
   "source": [
    "# Import Portrait and create token list\n",
    "portrait, portait_tokens = import_text(\"res/portrait.txt\")\n",
    "\n",
    "# Import Dubliners and create token list\n",
    "dubliners, dubliners_tokens = import_text(\"res/dubliners.txt\")\n",
    "\n",
    "joyce_intersection = (set(wake_tokens) & set(ulysses_tokens) & set(dubliners_tokens) & set(portait_tokens)) - set(stopwords)\n",
    "\n",
    "print(\"Number of words that occur in all four of Joyce's 'Novels':\")\n",
    "print(len(list(joyce_intersection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 3000 words that are in all four of Joyce's novels. Let's look at the frequency distribution of words in each of the books, then add all those scores together, and look at the list of the top 50 words that occur in all Joyce books. Below, we're only looking words that are longer than 5 letters because the short words are less interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little, 786\n",
      "father, 590\n",
      "street, 451\n",
      "always, 385\n",
      "though, 369\n",
      "thought, 315\n",
      "another, 306\n",
      "behind, 300\n",
      "course, 297\n",
      "turned, 293\n",
      "something, 293\n",
      "mother, 291\n",
      "towards, 290\n",
      "without, 281\n",
      "fellow, 273\n",
      "looked, 256\n",
      "better, 251\n",
      "morning, 244\n",
      "moment, 231\n",
      "called, 231\n",
      "nothing, 226\n",
      "passed, 225\n",
      "coming, 215\n",
      "things, 204\n",
      "looking, 204\n",
      "dublin, 197\n",
      "seemed, 181\n",
      "second, 178\n",
      "friend, 178\n",
      "people, 176\n",
      "brought, 174\n",
      "saying, 171\n",
      "perhaps, 170\n",
      "remember, 169\n",
      "walked, 169\n",
      "together, 167\n",
      "corner, 165\n",
      "others, 160\n",
      "slowly, 159\n",
      "evening, 157\n",
      "anything, 151\n",
      "enough, 150\n",
      "around, 150\n",
      "answered, 149\n"
     ]
    }
   ],
   "source": [
    "all_tokens = wake_tokens + ulysses_tokens + portait_tokens + dubliners_tokens\n",
    "all_tokens = [w for w in all_tokens if w in joyce_intersection]\n",
    "all_freq = nltk.FreqDist(all_tokens)\n",
    "for w in all_freq.most_common()[0:250]:\n",
    "    # We only look at words longer than 5 letter in length. Arbitrary, but more interesting this way.\n",
    "    if len(w[0]) > 5: \n",
    "        w_string = w[0] + \", \" + str(w[1])\n",
    "        print w_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HCE and ALP\n",
    "---------------------\n",
    "Hundreds of characters appear in *Finnegans Wake* but all of those characters are actually just manifestations or sub-manifestations of man and woman, husband and wife, mountain and river, space and time. Joyce calls them HCE and ALP.\n",
    "\n",
    "Let's use the power of [Regular Expressions](https://en.wikipedia.org/wiki/Regular_expression) to list all the different occurances of the initials HCE and ALP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Haroun Childeric Eggeberth                         he calmly extensolies.\n",
      " Hic cubat edilis.                                  How Copen-hagen ended.\n",
      " happinest childher everwere.                      \n",
      "How charmingly exquisite!\n",
      " Hither, craching eastuards,                        Hag Chivychas Eve,\n",
      " Here Comes Everybody.                              Habituels conspicuously emergent.\n",
      " H. C. Earwicker                                    he clearly expressed\n",
      " H. C. Earwicker,                                   He'll Cheat E'erawan\n",
      " haardly creditable edventyres                      haughty, cacuminal, erubescent\n",
      " Humpheres Cheops Exarchas,                         huge chain envelope,\n",
      " Hatches Cocks' Eggs,                               haught crested elmer,\n",
      " his corns either.                                  highly commendable exercise,\n",
      " high chief evervirens                              H2 C E3\n",
      " hagious curious encestor                           had claimed endright,\n",
      " Howforhim chirrupeth evereach-                     Homo Capite Erectus,\n",
      " He Can Explain,                                    Howke Cotchme Eye,\n",
      " Huffy Chops Eads,                                  hardily curio-sing entomophilust\n",
      " heptagon crystal emprisoms                         Hwang Chang evelytime;\n",
      " hoveth chieftains evrywehr,                        hereditatis columna erecta,\n",
      " hagion chiton eraphon;                             hallucination, cauchman, ectoplasm;\n",
      " hard cash earned                                   Hewitt Castello, Equerry,\n",
      " heavengendered, chaosfoedted, earthborn;           H. C. Endersen\n",
      " Henressy Crump Expolled,                           Huges Caput Earlyfouler.\n",
      " Her Chuff Exsquire!                                her calamity electrifies\n",
      " Hircus Civis Eblanensis!                           he can eyespy\n",
      " heather cliff emurgency                            Howarden's Castle, Englandwales.\n",
      " Hulker's cieclest elbownunsense.                   Housefather calls enthreateningly.\n",
      " human chain extends,                               Hocus Crocus, Esquilocus,\n",
      " his chthonic exterior                             \n",
      "Hoo cavedin earthwight\n",
      " Haud certo ergo.                                  \n",
      "Honour commercio's energy\n",
      "\n",
      "helm coverchaf emblem                              hce che ech,\n",
      " habby cyclic erdor                                 his craft ebbing,\n",
      " hof cullchaw end                                   Hengler's Circus Entertainment,\n",
      " harbour craft emittences,                          harmonic condenser enginium\n",
      " Howe cools Eavybrolly!                             Heave, coves, emptybloddy!\n",
      " hero chief explunderer                             her changeable eye\n",
      " Hermyn C. Entwhistle)                              His Cum-bulent Embulence,\n",
      " how comes ever                                     heaviest corpsus exemption)\n",
      " hugon come er-                                    \n",
      "Horkus chiefest ebblynuncies!\n",
      " Hence counsels Ecclesiast.                         Hung Chung Egglyfella\n",
      " hulm culms evurdyburdy.                            Hang coersion everyhow!\n",
      " hear, Caller Errin!)                               highly continental evenements,\n",
      " him, com-pound eyes                                His Christian's Em?\n",
      " Here Com-merces Enville.                           Helpless Corpses Enactment.\n",
      " home cooking every-time.                           home cured emigrant\n",
      " he could ever                                      Hunkalus Childared Easterheld.\n",
      " his coglionial expancian?                          him circuly. Evovae!\n",
      " Hodie casus esobhrakonton?                         hugger-knut cramwell energuman,\n",
      " Hotchkiss Culthur's Everready,                     his comfy estably\n",
      " Human Conger Eel!                                  Ho, croak, evildoer!\n",
      " handshakey congrandyoulikethems, ecclesency.       Haveth Childers Every\n",
      " Hery Crass Evohodie.                               her chastener ever\n",
      " huge Chesterfield elms                             Holiday, Christmas, Easter\n",
      " Horsehem coughs enough.                            heathen church emergency\n",
      " Hecklar's champion ethnicist.                      Herenow chuck english\n",
      " Heinz cans everywhere                              huskiest coaxing experimenter\n",
      " Humpfrey, champion emir,                           hugest commercial emporialist,\n",
      " honoured christmastyde easteredman.                horned cairns erge,\n",
      " hailed chimers' ersekind;                          holiday crowd encounter;\n",
      " hullow chyst ex-cavement;                          Homos Circas Elochlannensis!\n",
      " Hagiographice canat Ecclesia.                      Hump cumps Ebblybally!\n",
      "\n",
      "Health, chalce, endnessnessessity!                 hophaz-ards can effective\n",
      " helpyourselftoastrool cure's easy.                 hardest crux ever.\n"
     ]
    }
   ],
   "source": [
    "hce = re.findall(\"\\s[Hh]\\S*\\s[Cc]\\S*\\s[Ee]\\S*\", wake, re.U)\n",
    "for n in range(len(hce)//2):\n",
    "    print \"%-50s %s\" % (hce[n*2], hce[n*2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " askes lay. Phall                                   addle liddle phifie\n",
      " Apud libertinam parvulam                           a lugly parson\n",
      " along landed Paddy                                 a lady pack\n",
      " a lilyth, pull                                     annie lawrie promises\n",
      " a lady's postscript:                               arboro, lo petrusu.\n",
      " A Laugh-able Party,                                at length presuaded\n",
      " a lane picture                                     Any lucans, please?\n",
      " acta legitima plebeia,                             any luvial peatsmoor\n",
      " Annos longos patimur                               and leadlight panes.\n",
      " areyou looking-for Pearlfar                        Amy Licks Porter\n",
      " and lited, pleaded                                 a lovely park,\n",
      " are lovely, pitounette,                            and lice, pricking\n",
      " Amnis Limina Permanent)                            any lively purliteasy:\n",
      " a lunger planner's                                 a little present\n",
      " a loose past.                                      and Le PŠre\n",
      " Annushka Lutetiavitch Pufflovah,                   All Ladies' presents.\n",
      " and letters play                                   apes. Lights, pageboy,\n",
      " alla ludo poker                                    an litlee plads\n",
      " af liefest pose,                                   AND LIBERTINE. PROPE\n",
      " a lonely peggy,                                    appia lippia pluvaville,\n",
      " Art, literature, politics,                         American Lake Poetry,\n",
      " a luckybock, pledge                                Anna Lynchya Pourable\n",
      " anny livving plusquebelle,                         annapal livibel prettily\n",
      " a locally person                                   a lyncheon partyng\n",
      " Aquasancta Liffey Patrol                           and last pre-electric\n",
      " Auld Letty Plussiboots                             and Luse polkas,\n",
      " and love potients                                  artis litterarum-que patrona\n",
      " Anna Lynsha's Pekoe                                Annabella, Lovabella, Pullabella,\n",
      " asseveralation. Ladiegent, pals                    a libidous pickpuckparty\n",
      " and Lorencz Pattorn                                Appia Lippia Pluviabilla,\n",
      " Annshee lispes privily.                            ambling limfy peepingpartner,\n",
      " all ladies please                                  a lady!) pulling\n",
      "\n",
      "Alma Luvia, Pollabella.                            As leisure paces.\n"
     ]
    }
   ],
   "source": [
    "alp = re.findall(\"\\s[Aa]\\S*\\s[Ll]\\S*\\s[Pp]\\S*\", wake, re.U)\n",
    "for n in range(len(alp)//2):\n",
    "    print \"%-50s %s\" % (alp[n*2], alp[n*2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurances of HCE: 124\n",
      "Number of occurances of ALP: 67\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of occurances of HCE: \" + str(len(hce)))\n",
    "print(\"Number of occurances of ALP: \" + str(len(alp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Single ALP and HCE words\n",
    "What words that contain letters ALP, HCE, what about both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ALP words: 2706\n",
      "\n",
      "penisolate           oldparr              humptyhillhead       upturnpikepointandplace\n",
      "cata-pelting         apeal                appalling            leap\n",
      "multiplicab-les      staple               hierarchitec-titiptitoploftical baubletop\n",
      "municipal            plethora             bockalips            platterplate\n",
      "Shopalist            dalppling            parvulam             slaaps\n",
      "Lipoleumhat          pullupon-easyan      alps                 alps\n",
      "tallowscoop          plate                tallowscoop          tailoscrupp\n",
      "lamp                 preealittle          pouralittle          wipealittle\n",
      "pelfalittle          flapped              peacefugle           ilandiskippy\n",
      "scampulars           playing              planko               place's\n",
      "pall                 play                 parently             paisibly\n",
      "plain                perihelygangs        paxsealing           Fleapow\n",
      "pectoral             pillar               pleasurad            plaine\n",
      "plage                alp                  please               allaphbed\n",
      "please               please               place                plaus-ible\n",
      "palm                 toptypsical          leap                 wallops\n",
      "larpnotes            lamphouse            paly                 lilipath\n",
      "knavepaltry          campbells            panuncular           captol\n",
      "Wolkencap            Impalpabunt          lipalip              play\n",
      "swamplight           Healiopolis          Kapelavaster         players\n",
      "supershillelagh      palmsweat            paddyplanters        laps\n",
      "planter              pale                 falconplumes         appeals\n",
      "popular              mudapplication       lamp                 playing\n",
      "plaidboy             explain              leopard              wouldpay\n",
      "deadlop              place                paroqial             archipelago's\n",
      "wicklowpattern       respunchable         occupational         halltraps\n",
      "pivotal              proclaim             prefall              pleased\n",
      "spaniels             plain                plaid                lobstertrapping\n",
      "Haromphreyld         triptychal           pikebailer           Holmpatrick\n",
      "planted              andrewpaulmurphyc    place                punical\n",
      "inseparable          pantalime            holographs           spalpeens\n",
      "pleasant             populace             clap                 plaud\n",
      "special              play                 Napoleon             practical\n",
      "panelled             marbletopped         lamps                apparently\n",
      "caterpillar          capable              particularly         implicating\n",
      "topantically         place                pleaded              spontaneously\n",
      "partial              pleach               vollapluck           please\n",
      "physical             shrapnel             triplehydrad         floodplain\n",
      "postpuberal          slopingforward       gildthegap           scalp\n",
      "pawkytalk            please               respectable          platter\n",
      "nuptials             pispigliando         particular           primarily\n",
      "pecklapitschens      epistolear           personality          capable\n",
      "Chaplain             purpular             capalleens           Galop\n",
      "moltapuke            voltapuke            epickthalamorous     parabellum\n",
      "epipsychidically     applecheeks          maniplumbs           playable\n",
      "halfpast             apply                placed               planet's\n",
      "melomap              explain              professional         palesmen\n",
      "players              particularist        lappet               place\n",
      "privately            peacifold            plaudits             maypole\n",
      "klikkaklakkaklaskaklopatzklatschabattacreppycrotty-graddaghsemmihsammihnouithappluddyappladdypkonpkot penal                populace             scampitle\n"
     ]
    }
   ],
   "source": [
    "a_l_p = re.findall(r\"\\b(\\S*([alp])\\S*((?!\\2)([alp]))\\S*((?!(\\2|\\3))([alp]))\\S*)\\b\", wake, re.U)\n",
    "print \"Number of ALP words: \" + str(len(a_l_p)) + \"\\n\"\n",
    "# There are too many words so we're only going to look at the first 200\n",
    "a_l_p = a_l_p[0:200]\n",
    "for n in range(len(a_l_p)//4):\n",
    "    print \"%-20s %-20s %-20s %s\" % (a_l_p[n*4][0], a_l_p[n*4+1][0], a_l_p[n*4+2][0], a_l_p[n*4+3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HCE words: 2766\n",
      "\n",
      "thuartpeatrick       pftjschute           clashes              Whoyteboyce\n",
      "chance               cashels              pharce               pentschanjeuchy\n",
      "Childeric            hierarchitec-titiptitoploftical archers              cubehouse\n",
      "search               crunch-bracken       cheriffs             citherers\n",
      "trochees             hitched              Finiche              schlice\n",
      "sundyechosies        detch                scotcher             Touchole\n",
      "sheltershock         Belchum              Belchum              Belchum\n",
      "Belchum              Belchum              blooches             wrothschields\n",
      "whence               childher             coacher's            brooches\n",
      "allmicheal           muchears             phace                Herrschuft\n",
      "ketch                scentbreeched        choose               mechanics\n",
      "each                 chabelshovel-ler     Mitchel              lichening\n",
      "Marchessvan          quickenshoon         Ballyaughacleeagh-bally charged\n",
      "selfstretches        choosed              childsfather         nuncheon\n",
      "chorley              excheck              poached              hence\n",
      "Stench               thanacestross        attachment           bitches\n",
      "reaching             earthcrust           creakish             Toucheaterre\n",
      "cotched              milch-camel          charter              closeth\n",
      "chopwife             mischievmiss         besch                kickaheeling\n",
      "niece-of-his-inlaw   punched              changers             hurricane\n",
      "watercloth           bench                enfranchisable       kerchief\n",
      "cheeks               toethpicks           chewed               chepped\n",
      "Maccullaghmore       archgoose            headboddylwatcher    chempel\n",
      "dinnerchime          cherub               cheek                schoolbelt\n",
      "choose               twicenightly         Fetch                fetch\n",
      "misches              tabouretcushion      enchantement         allavalonche\n",
      "chickle              chuckle              Pervenche            sepulchres\n",
      "switches             bennbranch           chimpney             theatrocrat\n",
      "cohalething          voucherfors          archipelago's        schooner\n",
      "waxenwench           repreaching          respunchable         chalked\n",
      "authenticated        ethnarch             hotface              perch\n",
      "moustaches           whitelock            Michael              Michael\n",
      "cheerycherrily       andrewpaulmurphyc    changeth             broadstretched\n",
      "kerchief             clawhammers          characters           christlikeness\n",
      "chargehard           Roche                pleach               hence\n",
      "chloereydes          hideinsacks          schulder             chronometrum\n",
      "hygienic             checkself            mannleich            ethics\n",
      "secondmouth          belcher              Peach                Phenice–Bruerie\n",
      "cherished            chee                 speech               pecklapitschens\n",
      "layteacher           orthophonethics      stablecloth          Packenham's\n",
      "chanced              racenight            choicest             alcoherently\n",
      "epickthalamorous     cashdraper's         discharged           beachbusker\n",
      "melancholia          cocklehat            Roche                epipsychidically\n",
      "applecheeks          chilled              crewth               prothetic\n",
      "whackfolthediddlers  luncheon             richer               cocoahouse\n",
      "chief                chronicler           airwhackers          search\n",
      "exchanging           chisellers           cloudhued            Poulichinello\n",
      "chalice              chantied             chorussed            christened\n",
      "klikkaklakkaklaskaklopatzklatschabattacreppycrotty-graddaghsemmihsammihnouithappluddyappladdypkonpkot schemes              coaches              change\n",
      "chewing              chicken-pox          chambers             bucketshop\n"
     ]
    }
   ],
   "source": [
    "h_c_e = re.findall(r\"\\b(\\S*([hce])\\S*((?!\\2)([hce]))\\S*((?!(\\2|\\3))([hce]))\\S*)\\b\", wake, re.U)\n",
    "print \"Number of HCE words: \" + str(len(h_c_e)) + \"\\n\"\n",
    "# There are too many words so we're only going to look at the first 200\n",
    "h_c_e = h_c_e[0:200]\n",
    "for n in range(len(h_c_e)//4):\n",
    "    print \"%-20s %-20s %-20s %s\" % (h_c_e[n*4][0], h_c_e[n*4+1][0], h_c_e[n*4+2][0], h_c_e[n*4+3][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so there are a ton of HCE and ALP words. So many so that searching for that patterning seems kinda fruitless\n",
    "\n",
    "Instead what if we look for words that have some combination of all of those letters\n",
    "\n",
    "## HCE + ALP words\n",
    "\n",
    "We'll modify out earlier regular expression to return these words.\n",
    "\n",
    "If you'd like to see a description of what's going on in that crazy long regex below, follow [this link](https://regex101.com/r/oH9rG9/1) and click 'Explanation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HCEALP words: 87\n",
      "\n",
      "hierarchitec-titiptitoploftical archipelago's        respunchable         andrewpaulmurphyc\n",
      "pleach               pecklapitschens      epickthalamorous     epipsychidically\n",
      "applecheeks          accomplished         placewheres          chapelgoers\n",
      "morphomelosophopancreates pathetically         Isitachapel–Asitalukin place-hider\n",
      "patchpurple          houdingplaces        chapel               blacksheep\n",
      "hyperchemical        accomplished         metropoliarchialisation accomplished\n",
      "palypeachum          unspeechably         chaplet              accom-plished\n",
      "pupil-teacher's      cheekadeekchimple    steeplechange        pupilteachertaut\n",
      "blockcheap           howdrocephalous      parishclerks         Brighten-pon-the-Baltic\n",
      "stuumplecheats       Makehal-pence        Holophullopopu-lace  speechsalver's\n",
      "heliotropical        fellow-chap          handcomplishies      hopeygoalucrey\n",
      "parchels             accomplishment       Estchapel            halfpricers\n",
      "polemarch            kalospintheochromatokreening chaptel              chapell-ledeosy\n",
      "chaplets             ptchjelasys          polthronechair       chapplie\n",
      "pulcherman           chapel               plumpchake           chap-lets\n",
      "withumpronouceable   humeplace            peachumpidgeonlover  cheaply\n",
      "hidingplace          choreopiscopally     blepharospasmockical psychoanolised\n",
      "bach-spilled         chapelry             halfprice            chapel\n",
      "busspleaches         Archfieldchaplain    sleepingchambers     chapelofeases\n",
      "littleeasechapel     placehunter          chapelgoer           cheaply\n",
      "showplace            archipelago          blackshape           blackinwhitepaddynger\n"
     ]
    }
   ],
   "source": [
    "h_c_e_a_l_p = re.findall(r\"\\b(\\S*([hcealp])\\S*((?!\\2)([hcealp]))\\S*((?!(\\2|\\3))([hcealp]))\\S*((?!(\\2|\\3|\\5))([hcealp]))\\S*((?!(\\2|\\3|\\5|\\8))([hcealp]))\\S*((?!(\\2|\\3|\\5|\\8|\\11))([hcealp]))\\S*)\\b\", wake, re.U)\n",
    "\n",
    "# This contains all the thunder words, we're gonna remove them\n",
    "h_c_e_a_l_p = [w for w in h_c_e_a_l_p if len(w[0]) < 70]\n",
    "print \"Number of HCEALP words: \" + str(len(h_c_e_a_l_p)) + \"\\n\"\n",
    "for n in range(len(h_c_e_a_l_p)//4):\n",
    "    print \"%-20s %-20s %-20s %s\" % (h_c_e_a_l_p[n*4][0], h_c_e_a_l_p[n*4+1][0], h_c_e_a_l_p[n*4+2][0], h_c_e_a_l_p[n*4+3][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams and Trigrams\n",
    "---------------------\n",
    "HCE and ALP may be frequently occuring initials, lets look at frequently occuring phrases and see if anything interesting arises.\n",
    "\n",
    "Bigrams are pairs of words that occur side-by-side. Trigrams are triplets of words that occur side-by-side.\n",
    "\n",
    "Below we generate a list of the most frequently occuring bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====BIGRAMS=====\n",
      "\"let us\", 37\n",
      "\"wo n't\", 33\n",
      "\"ca n't\", 32\n",
      "\"would n't\", 26\n",
      "\"poor old\", 16\n",
      "\"could n't\", 14\n",
      "\"come back\", 12\n",
      "\"anna livia\", 12\n",
      "\"ah ho\", 12\n",
      "\"shaun replied\", 11\n",
      "\"o o\", 10\n",
      "\"ay ay\", 10\n",
      "\"every time\", 9\n",
      "\"tell us\", 9\n",
      "\"grand old\", 9\n",
      "\"n't say\", 8\n",
      "\"hide seek\", 8\n",
      "\"one time\", 8\n",
      "\"would like\", 8\n",
      "\"n't know\", 8\n",
      "\"old man\", 7\n",
      "\"right enough\", 7\n",
      "\"one day\", 7\n",
      "\"wait till\", 7\n",
      "\"ho ho\", 7\n",
      "\"say nothing\", 7\n",
      "\"ever since\", 7\n",
      "\"good old\", 7\n",
      "\"n't forget\", 6\n",
      "\"n't tell\", 6\n",
      "\n",
      "\n",
      "=====TRIGRAMS=====\n",
      "\"o o o\", 7\n",
      "\"jarl von hoother\", 5\n",
      "\"nin nin nin\", 4\n",
      "\"seek hide seek\", 4\n",
      "\"hide seek hide\", 4\n",
      "\"ho ho ho\", 3\n",
      "\"whoishe whoishe whoishe\", 3\n",
      "\"meeter cat wife\", 3\n",
      "\"many many many\", 3\n",
      "\"steady steady steady\", 3\n",
      "\"order order order\", 3\n",
      "\"ah dearo dearo\", 3\n",
      "\"jarl van hoother\", 3\n",
      "\"dearo dearo dear\", 3\n",
      "\"hear o hear\", 3\n",
      "\"fore shalt thou\", 2\n",
      "\"hip champouree hiphip\", 2\n",
      "\"hoke hoke hoke\", 2\n",
      "\"two plays punk\", 2\n",
      "\"ting ting ting\", 2\n",
      "\"variants katey sherratt\", 2\n",
      "\"rally o rally\", 2\n",
      "\"big white harse\", 2\n",
      "\"au aue ha\", 2\n",
      "\"hee hee hee\", 2\n",
      "\"old matt gregory\", 2\n",
      "\"n k b\", 2\n",
      "\"let us hear\", 2\n",
      "\"piercey piercey piercey\", 2\n",
      "\"bad bleak boy\", 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate list of bigram tuples from wake_tokens\n",
    "bgs = nltk.bigrams(wake_tokens)\n",
    "# Generate list of trigram tuples from wake_tokens\n",
    "tgs = nltk.trigrams(wake_tokens)\n",
    "\n",
    "# Define a function which takes a list of ngram tuples NGRAM\n",
    "# returns a that list with any tuples that contain stopwords or punctuation removed.\n",
    "# this is how we get actually interesting phrases\n",
    "def ngramsFilter(ngram):\n",
    "    ngrams_filtered = []\n",
    "    for tup in ngram:\n",
    "        flag = True\n",
    "        for w in tup:\n",
    "            if w in stopwords:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            ngrams_filtered.append(tup)\n",
    "    return ngrams_filtered\n",
    "\n",
    "# Map each bigram to its number of occurances throughout the tokens\n",
    "fdist_bigrams = nltk.FreqDist(ngramsFilter(bgs))\n",
    "# Map each trigram to its number of occurances throughout the tokens\n",
    "fdist_trigrams =nltk.FreqDist(ngramsFilter(tgs))\n",
    "\n",
    "print(\"=====BIGRAMS=====\")\n",
    "for tup in fdist_bigrams.most_common(30):\n",
    "    print('\"' + tup[0][0] + \" \" + tup[0][1] + '\", ' + str(tup[1]))\n",
    "print(\"\\n\")\n",
    "print(\"=====TRIGRAMS=====\")\n",
    "for tup in fdist_trigrams.most_common(30):\n",
    "    print('\"' + tup[0][0] + \" \" + tup[0][1] + \" \" + tup[0][2] + '\", ' + str(tup[1]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Queen's English\n",
    "---------------------\n",
    "*Finnegans Wake* seems inscrutable but many Joyceans say that the best guide to the wake is just a comprehensive English dictionary. Let's see just how many words in Finnegans are actually in English.\n",
    "\n",
    "We'll import a list of English words then see if we can find each word in that English word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import a list of all English Words\n",
    "en_words, en_words_tokens = import_text('res/en-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of English words to total words: 0.778632018189\n"
     ]
    }
   ],
   "source": [
    "#EN_WAKE is the full text of Finnegans Wake with all non-English words removed\n",
    "en_wake = [w for w in wake_tokens if w.lower() in en_words_tokens]\n",
    "en_ratio = len(en_wake) / len(wake_tokens)\n",
    "print(\"Ratio of English words to total words: \" + str(en_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 77.9% of the words in *Finnegans Wake* are valid (in the dictionary) English words.\n",
    "\n",
    "That above computation is pretty nasty though. Because it relies on a nested for-loop it runs in O(n<sup>2</sup>) time.\n",
    "\n",
    "We have a set of unique words from above. Let's just use set intersection to check quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of unique English words to total unique words: 0.334550195567\n"
     ]
    }
   ],
   "source": [
    "# EN_WAKE_SET is a list of every unique English word in Finnegans Wake\n",
    "wake_tokens_set =  set(wake_tokens)\n",
    "en_wake_set = list(set(en_words_tokens) & wake_tokens_set)\n",
    "unique_en_ratio = len(en_wake_set) / len(wake_tokens_set)\n",
    "print(\"Ratio of unique English words to total unique words: \" + str(unique_en_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, O(n). That's nice. We'll use this again later.\n",
    "\n",
    "Of all the different words that occur in *Finnegans Wake* only 32.5% of those words are plain English!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Most Common Non-English Words\n",
    "Working with sets is nice and all but it destroys our frequency information. So if we, for instance, would like to know which are the most commonly occuring non-english words in *The Wake* then we have to use the inverse list of `eng_wake` from above.\n",
    "\n",
    "Below wer are only looking at words that are longer than two letters because the two letter words aren't so interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NON_ENG_WAKE is the full text of Finnegans Wake with all English words removed\n",
    "set_en_tokens = set(en_words_tokens)\n",
    "non_en_wake = [w for w in wake_tokens if w.lower() not in set_en_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n't, 289             shaun, 46            mrs, 43              sayd, 35\n",
      "willingdone, 24      o'er, 24             taff, 20             kevin, 20\n",
      "sagd, 19             shem, 19             mookse, 18           shee, 17\n",
      "browne, 16           ing, 16              jaun, 15             hosty, 15\n",
      "livia, 15            arrah, 15            yous, 15             jinnies, 14\n",
      "kersse, 14           juva, 13             muta, 13             humphrey, 13\n",
      "marcus, 13           sazd, 13             nolan, 13            rann, 13\n",
      "talis, 12            1132, 12             earwicker, 12        lyons, 11\n",
      "moy, 11              tay, 11              yur, 10              lucan, 10\n",
      "thon, 10             lipoleums, 10        anny, 10             e'er, 9\n",
      "blong, 9             jarl, 9              les, 9               dee, 9\n",
      "est, 9               honuphrius, 9        glugg, 9             anita, 9\n",
      "nin, 9               mor, 9               hoother, 9           dook, 8\n",
      "efter, 8             med, 8               rere, 8              gracehoper, 8\n",
      "bruno, 8             sur, 8               zinzin, 8            roderick, 8\n",
      "tum, 8               caseous, 8           tarpey, 8            ne'er, 8\n",
      "gunne, 8             meself, 8            pon, 8               maggy, 8\n",
      "liv, 8               burrus, 8            lally, 8             lang, 7\n",
      "o'clock, 7           noo, 7               begor, 7             prankquean, 7\n",
      "yer, 7               bom, 7               whoishe, 7           twentynine, 7\n",
      "kish, 7              magnus, 7            dearo, 7             magrath, 7\n",
      "coppinger, 7         rede, 7              kelly, 7             chee, 7\n",
      "kate, 7              poghue, 7            ondt, 7              uns, 7\n",
      "una, 7               brinabride, 7        hoo, 7               neighbour, 7\n",
      "o'connell, 7         guinness, 7          longa, 6             davy, 6\n",
      "hal, 6               dood, 6              begge, 6             hoose, 6\n",
      "boyne, 6             andy, 6              erse, 6              ser, 6\n",
      "lala, 6              hume, 6              wot, 6               ned, 6\n",
      "ist, 6               piff, 6              knowed, 6            mand, 6\n",
      "het, 6               hek, 6               honourable, 6        lou, 6\n",
      "liffey, 6            maggies, 6           finnegan, 6          poo, 6\n",
      "forenenst, 6         alla, 6              bould, 6             lil, 6\n",
      "harse, 6             moe, 6               fing, 6              dans, 6\n",
      "airish, 5            olde, 5              lipoleum, 5          hugh, 5\n",
      "ould, 5              kinsella, 5          howth, 5             rota, 5\n",
      "mee, 5               brune, 5             doyle, 5             quoniam, 5\n",
      "pree, 5              liss, 5              ashe, 5              lep, 5\n",
      "e'en, 5              belchum, 5           ney, 5               clontarf, 5\n",
      "wist, 5              a.d., 5              pive, 5              magravius, 5\n",
      "airly, 5             lucas, 5             poors, 5             thom, 5\n",
      "eva, 5               ind, 5               poss, 5              hims, 5\n",
      "owld, 5              und, 5               gael, 5              gon, 5\n",
      "twy, 5               fane, 5              eugenius, 5          schwrites, 5\n",
      "fingal, 5            thirtytwo, 5         vico, 5              hom, 5\n",
      "hou, 5               hoy, 5               tunc, 5              sall, 5\n",
      "mear, 5              wisha, 5             haha, 5              allso, 5\n",
      "bawn, 5              ida, 5               olaf, 5              chinchin, 5\n",
      "ville, 5             bis, 5               heer, 5              fas, 5\n",
      "schott, 5            hin, 5               dumm, 5              wather, 5\n",
      "tak, 5               ching, 4             nolans, 4            boose, 4\n",
      "ond, 4               vuk, 4               sant, 4              doon, 4\n",
      "latearly, 4          walters, 4           mathew, 4            saft, 4\n",
      "slee, 4              cong, 4              hans, 4              usque, 4\n",
      "huges, 4             noe, 4               poghuing, 4          wohl, 4\n",
      "iren, 4              mes, 4               myles, 4             liddle, 4\n",
      "bett, 4              o'brien, 4           nuvoletta, 4         deff, 4\n",
      "backwords, 4         mester, 4            muy, 4               brandnew, 4\n",
      "laddy, 4             crumlin, 4           andt, 4              jeremias, 4\n",
      "jove, 4              soll, 4              whu, 4               puffpuff, 4\n",
      "champouree, 4        dora, 4              buckley, 4           fou, 4\n"
     ]
    }
   ],
   "source": [
    "non_en_wake_dist = nltk.FreqDist(non_en_wake)\n",
    "\n",
    "# We're only looking at the top 300\n",
    "most_common = [w for w in non_en_wake_dist.most_common()[0:300] if len(w[0]) > 2]\n",
    "for n in range(len(most_common)//4):\n",
    "    print \"%-20s %-20s %-20s %s\" % (most_common[n*4][0] + \", \" + str(most_common[n*4][1]) ,\n",
    "                                    most_common[n*4+1][0] + \", \" + str(most_common[n*4+1][1]), \n",
    "                                    most_common[n*4+2][0] + \", \" + str(most_common[n*4+2][1]), \n",
    "                                    most_common[n*4+3][0] + \", \" + str(most_common[n*4+3][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Full English\n",
    "Some sentences in *the Wake* are full of purely English words. Let's find them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
